# Kaggle Titanic - data science solutions
## DSC_SOOKMYUNG 머신러닝 스터디 권은지

### 참고 사이트
+ https://www.kaggle.com/eunjikwon99/titanic-data-science-solutions/edit
+ https://writeren.tistory.com/68
+ https://lsjsj92.tistory.com/426

## 1. 기본적인 데이터 확인
+ 필요한 모듈 import 
    ``` python
    	# data analysis and wrangling
	import pandas as pd
	import numpy as np
	import random as rnd

	# visualization
	import seaborn as sns
	import matplotlib.pyplot as plt
	%matplotlib inline

	# machine learning
	from sklearn.linear_model import LogisticRegression
	from sklearn.svm import SVC, LinearSVC
	from sklearn.ensemble import RandomForestClassifier
	from sklearn.neighbors import KNeighborsClassifier
	from sklearn.naive_bayes import GaussianNB
	from sklearn.linear_model import Perceptron
	from sklearn.linear_model import SGDClassifier
	from sklearn.tree import DecisionTreeClassifier```

+ 데이터 불러오기
    ```python
	train_df = pd.read_csv('../input/train.csv')
	test_df = pd.read_csv('../input/test.csv')
	combine = [train_df, test_df]```

+ 데이터 형태 확인
    + columns 확인
	`print(train_df.columns.values)`

    + 데이터 형태 확인
        ```python
	train_df.head()
	train_df.tail()

	train_df.info()
	print('_'*40)
	test_df.info()```

    + 데이터 요약
	`train_df.describe()`


+ groupby를 이용한 그룹 별 생존 여부 확인
    + Pclass & Survived
       ```python
	train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)```

    + Sex & Survived
       ```ptyhon
	train_df[["Sex", "Survived"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)```

    + SibSp & Survived
       ```ptyhon
	train_df[["SibSp", "Survived"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)```

    + Parch & Survived
       ```ptyhon
	train_df[["Parch", "Survived"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)```

+ 데이터 시각화
    + FacetGrid 히스토그램를 이용한 Age와 Survived 사이의 관계
       ```python
	g = sns.FacetGrid(train_df, col='Survived')
	g.map(plt.hist, 'Age', bins=20)```

    + FacetGrid 히스토그램를 이용한 Pclass와 Age, Survived 사이의 관계
       ```python
	grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)
	grid.map(plt.hist, 'Age', alpha=.5, bins=20)
	grid.add_legend();```

    + FacetGrid pointplot를 이용한 Pclass와 Sex, Survived 사이의 관계
       ```python
	grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)
	grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')
	grid.add_legend()```

    + FacetGrid barplot를 이용한 Sex와 Fare, Survived 사이의 관계
       ```python
	grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)
	grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)
	grid.add_legend()```

    + FacetGrid pointplot를 이용한 Pclass와 Age, Survived 사이의 관계
       ```python
	```

+ 데이터 정리
    + 이름의 title 분류
       ```python
	for dataset in combine:
    	   dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)

	pd.crosstab(train_df['Title'], train_df['Sex'])
	```

    + 잘나오지 않는 이름의 title을 Rare로 합체 후 확인
       ```python
	for dataset in combine:
    	    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\
 	    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')

    	    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
    	    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
    	    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')
    
	train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()```

+ 모델별 점수 확인 및 비교
    + LogisticRegression
       ```python
	logreg = LogisticRegression()
	logreg.fit(X_train, Y_train)
	Y_pred = logreg.predict(X_test)
	acc_log = round(logreg.score(X_train, Y_train) * 100, 2)
	acc_log```

    + SVC
       ```python
	svc = SVC()
	svc.fit(X_train, Y_train)
	Y_pred = svc.predict(X_test)
	acc_svc = round(svc.score(X_train, Y_train) * 100, 2)
	acc_svc```

    + KNN
       ```python
	knn = KNeighborsClassifier(n_neighbors = 3)
	knn.fit(X_train, Y_train)
	Y_pred = knn.predict(X_test)
	acc_knn = round(knn.score(X_train, Y_train) * 100, 2)
	acc_knn
	```

    + GaussianNB
       ```python
	gaussian = GaussianNB()
	gaussian.fit(X_train, Y_train)
	Y_pred = gaussian.predict(X_test)
	acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)
	acc_gaussian```

    + Perceptron
       ```python
	perceptron = Perceptron()
	perceptron.fit(X_train, Y_train)
	Y_pred = perceptron.predict(X_test)
	acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)
	acc_perceptron
	```

    + Linear SVC
       ```python
	linear_svc = LinearSVC()
	linear_svc.fit(X_train, Y_train)
	Y_pred = linear_svc.predict(X_test)
	acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)
	acc_linear_svc```

    + SGDClassifier
       ```python
	sgd = SGDClassifier()
	sgd.fit(X_train, Y_train)
	Y_pred = sgd.predict(X_test)
	acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)
	acc_sgd```

    + Decision Tree
       ```python
	decision_tree = DecisionTreeClassifier()
	decision_tree.fit(X_train, Y_train)
	Y_pred = decision_tree.predict(X_test)
	acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)
	acc_decision_tree```

    + RandomForest
       ```python
	random_forest = RandomForestClassifier(n_estimators=100)
	random_forest.fit(X_train, Y_train)
	Y_pred = random_forest.predict(X_test)
	random_forest.score(X_train, Y_train)
	acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)
	acc_random_forest```

    + Score 높은 순으로 나열
       ```python
	models = pd.DataFrame({
    	    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 
              	    'Random Forest', 'Naive Bayes', 'Perceptron', 
              	    'Stochastic Gradient Decent', 'Linear SVC', 
              	    'Decision Tree'],
    	    'Score': [acc_svc, acc_knn, acc_log, 
              	  acc_random_forest, acc_gaussian, acc_perceptron, 
              	  acc_sgd, acc_linear_svc, acc_decision_tree]})
	models.sort_values(by='Score', ascending=False)```
    