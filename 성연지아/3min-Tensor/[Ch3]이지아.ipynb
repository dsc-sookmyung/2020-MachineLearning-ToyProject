{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorlow library\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_3:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant('Hello, Tensorflow!')\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3-1 텐서 \n",
    " : 텐서플로에서 가장 기본적인 자료형으로 Rank와 Shape의 개념을 가짐\n",
    " \n",
    " * 랭크(Rank)\n",
    "  * 차원의 수를 나타냄 \n",
    "  * 0이면 스칼라, 1이면 벡터, 2는 행령 3이상은 n-Tensor (n차원 텐서)\n",
    "  \n",
    " * 셰이프(Shape)\n",
    "  * 각 차원의 요소 개수 ( 텐서의 구조를 설명 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_2:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a= tf.constant(10)\n",
    "b= tf.constant(40)\n",
    "c=tf.add(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프로그램의 구조\n",
    "\n",
    "#### 1. 그래프 생성\n",
    ": 텐서와 텐서의 연사들을 먼저 정의하여 그래프를 만든다\n",
    "\n",
    "#### 2. 그래프 실행 \n",
    ": 원하는 시점에 실제 연산을 수행 => Session 안에서 이루어짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, Tensorflow!'\n",
      "[10, 40, 50]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a,b,c]))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 플레이스홀더와 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 플레이스홀더 \n",
    ": 그래프에 사용할 입력값을 나중에 받기 위해 사용하는 parameter(매개변수)\n",
    "\n",
    "#### 2. 변수\n",
    ": 그래프를 최적화 하는 용도로 학습 함수들이 학습한 결과를 갱신하기 위함 \n",
    " * 이 변수 값들이 신경망의 성능을 좌우함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# (?,3) 모양의 float32 자료형을 가진 텐서 생성 \n",
    "X = tf.placeholder(tf.float32,[None,3])\n",
    "print(X)\n",
    "\n",
    "x_data = [[1,2,3],[4,5,6]]\n",
    "\n",
    "# [3,2] 행렬 형태의 탠서\n",
    "## tf.random_normal 함수로 정규분포의 무작위 값으로 초기화\n",
    "W= tf.Variable(tf.random_normal([3,2]))\n",
    "\n",
    "# [2,1] 행렬 형태의 텐서 \n",
    "b= tf.Variable(tf.random_normal([2,1]))\n",
    "\n",
    "#행렬곱 연산 (tf.matmul)\n",
    "expr = tf.matmul(X,W)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬곱 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행렬곱 A X B에 대하여, 행렬 A의 열 수와 행렬 B의 행 수는 같아야 한다.\n",
    "\n",
    "* 행렬곱 A X B를 계산한 행렬 AB의 크기는 A의 행 개수와 B의 열 개수가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====x_data====\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "====W====\n",
      "[[ 1.4707674  -1.7815707 ]\n",
      " [ 0.17070957  1.5098804 ]\n",
      " [-1.1469615  -1.4239373 ]]\n",
      "====b====\n",
      "[[-0.36351264]\n",
      " [ 0.71006644]]\n",
      "====expr====\n",
      "[[-1.9922105 -3.3971343]\n",
      " [ 0.5649148 -7.410438 ]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "#앞에서 정의한 변수들을 초기화하는 함수(global_variables_initiallizer())\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"====x_data====\")\n",
    "print(x_data)\n",
    "print(\"====W====\")\n",
    "print(sess.run(W))\n",
    "print(\"====b====\")\n",
    "print(sess.run(b))\n",
    "print(\"====expr====\")\n",
    "#feed_dict : 그래프를 실행할 때 사용할 입력값 지정(변수임)\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3 선형 회귀 모델 구현하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 선형 회귀란( linear regression)\n",
    ": 주어진 x와 y의 값을 가지고 서로 간의 **관계(상관 관계)를 파악**하는 것 \\\n",
    "=> 즉, 어떤 입력값에 대한 출력을 예측하는 것이다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data =[1,2,3]\n",
    "y_data =[1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "X = tf.placeholder(tf.float32, name =\"X\")\n",
    "Y = tf.placeholder(tf.float32, name =\"Y\")\n",
    "\n",
    "# W:가중치 , b: 편향 \n",
    "hypothesis = W * X + b\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 손실함수 (loss function)\n",
    ": 한 쌍의 (x,y)의 데이터에 대한 손실값을 계산하는 함수 \n",
    "\n",
    "* **손실값** \\\n",
    "실제값과 모델로 예측한 값이 얼마나 차이가 나는가 나타내는 값 \\\n",
    " \\: 손실값이 작을수록 그 모델이 X와 Y의 관계 잘 설명하고 있다는 의미 \n",
    "    + 손실값으로는 예측값과 실제값의 거리를 가장 많이 사용함\n",
    "\n",
    "\n",
    "* **비용** \\\n",
    ":손실을 전체 데이터에 대해 구한 경우 \n",
    "    + 모든 데이터에 대한 손실값의 평균을 구함\n",
    "\n",
    "\n",
    "* **학습**\n",
    ": 변수들의 값을 다양하게 넣어 계산해보면서 손실값을 최소화하는 W와 b의 값을 구하는 것 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 최적화함수 - 경사하강법(gradient descent)\n",
    ": 손실값을 최소화하는 연산 그래프 생성 \n",
    "\n",
    "* **최적화 함수란?** \\\n",
    ": 가중치와 편향 값을 변경해가면서 손실값을 최소화하는 가장 최적화된 가중치와 편향 값을 찾아주는 함수\n",
    "\n",
    "\n",
    "* **경사하강법** \\\n",
    ": 함수의 기울기를 구하고 기울기가 낮은 쪽으로 계속 이동시키면서 최적의 값을 찾아나감\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate(학습률-하이퍼파라미터): 얼마나 학습을 급하게 할 것인가를 결정\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "#최적화를 수행하는 그래프 \n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4985831 [0.75730896] [0.6282697]\n",
      "1 0.05968282 [0.7325127] [0.59969217]\n",
      "2 0.05188799 [0.7422907] [0.58674866]\n",
      "3 0.04936405 [0.74811995] [0.57248265]\n",
      "4 0.0470185 [0.75421494] [0.5587381]\n",
      "5 0.044785053 [0.7601191] [0.54530454]\n",
      "6 0.042657737 [0.76588607] [0.532196]\n",
      "7 0.040631484 [0.771514] [0.5194023]\n",
      "8 0.03870145 [0.7770067] [0.5069163]\n",
      "9 0.03686309 [0.7823672] [0.49473035]\n",
      "10 0.03511207 [0.78759897] [0.48283738]\n",
      "11 0.033444226 [0.792705] [0.47123033]\n",
      "12 0.031855617 [0.79768825] [0.45990226]\n",
      "13 0.03034244 [0.8025516] [0.4488465]\n",
      "14 0.028901147 [0.8072982] [0.43805653]\n",
      "15 0.027528316 [0.8119306] [0.42752597]\n",
      "16 0.026220703 [0.81645167] [0.41724855]\n",
      "17 0.024975205 [0.820864] [0.40721816]\n",
      "18 0.023788853 [0.82517034] [0.3974289]\n",
      "19 0.022658864 [0.8293731] [0.38787496]\n",
      "20 0.021582572 [0.8334749] [0.3785507]\n",
      "21 0.020557376 [0.83747804] [0.3694506]\n",
      "22 0.019580884 [0.841385] [0.36056927]\n",
      "23 0.018650759 [0.845198] [0.3519014]\n",
      "24 0.017764838 [0.8489193] [0.34344193]\n",
      "25 0.016921014 [0.85255116] [0.33518583]\n",
      "26 0.01611725 [0.8560958] [0.3271282]\n",
      "27 0.015351658 [0.8595551] [0.31926426]\n",
      "28 0.01462246 [0.8629314] [0.3115894]\n",
      "29 0.013927854 [0.86622626] [0.30409893]\n",
      "30 0.013266284 [0.86944216] [0.29678863]\n",
      "31 0.012636124 [0.8725807] [0.28965405]\n",
      "32 0.012035911 [0.8756438] [0.28269094]\n",
      "33 0.011464176 [0.8786332] [0.27589524]\n",
      "34 0.010919631 [0.88155085] [0.2692629]\n",
      "35 0.010400931 [0.88439816] [0.26278996]\n",
      "36 0.009906891 [0.8871772] [0.2564727]\n",
      "37 0.00943629 [0.8898894] [0.25030726]\n",
      "38 0.008988065 [0.8925364] [0.24429004]\n",
      "39 0.00856112 [0.8951197] [0.23841746]\n",
      "40 0.008154467 [0.897641] [0.23268607]\n",
      "41 0.0077671255 [0.9001016] [0.22709244]\n",
      "42 0.0073981774 [0.90250313] [0.22163332]\n",
      "43 0.0070467466 [0.90484685] [0.21630539]\n",
      "44 0.0067120357 [0.9071343] [0.21110557]\n",
      "45 0.0063932077 [0.9093667] [0.20603074]\n",
      "46 0.006089516 [0.91154546] [0.20107788]\n",
      "47 0.005800267 [0.9136719] [0.19624412]\n",
      "48 0.005524753 [0.91574717] [0.19152655]\n",
      "49 0.005262317 [0.9177725] [0.18692236]\n",
      "50 0.0050123637 [0.91974926] [0.1824289]\n",
      "51 0.004774263 [0.9216784] [0.17804343]\n",
      "52 0.004547495 [0.9235612] [0.17376338]\n",
      "53 0.0043314747 [0.92539877] [0.16958623]\n",
      "54 0.0041257325 [0.9271921] [0.16550948]\n",
      "55 0.0039297557 [0.9289424] [0.16153076]\n",
      "56 0.0037430918 [0.93065053] [0.15764765]\n",
      "57 0.003565288 [0.9323176] [0.1538579]\n",
      "58 0.0033959383 [0.9339447] [0.15015927]\n",
      "59 0.0032346195 [0.93553257] [0.14654952]\n",
      "60 0.003080974 [0.9370824] [0.1430266]\n",
      "61 0.0029346228 [0.9385948] [0.13958831]\n",
      "62 0.0027952318 [0.940071] [0.13623272]\n",
      "63 0.0026624613 [0.9415117] [0.1329578]\n",
      "64 0.0025359855 [0.94291764] [0.12976158]\n",
      "65 0.0024155236 [0.94428986] [0.12664218]\n",
      "66 0.002300785 [0.9456291] [0.12359781]\n",
      "67 0.0021914986 [0.94693613] [0.12062659]\n",
      "68 0.002087397 [0.9482117] [0.11772681]\n",
      "69 0.001988249 [0.9494567] [0.11489674]\n",
      "70 0.0018938044 [0.95067173] [0.11213471]\n",
      "71 0.0018038483 [0.95185757] [0.10943906]\n",
      "72 0.0017181592 [0.95301485] [0.10680822]\n",
      "73 0.0016365475 [0.95414436] [0.10424064]\n",
      "74 0.0015588143 [0.95524675] [0.10173477]\n",
      "75 0.0014847643 [0.9563225] [0.0992891]\n",
      "76 0.0014142363 [0.9573725] [0.09690228]\n",
      "77 0.0013470595 [0.9583972] [0.09457282]\n",
      "78 0.0012830765 [0.9593973] [0.09229936]\n",
      "79 0.0012221268 [0.9603734] [0.09008055]\n",
      "80 0.0011640753 [0.961326] [0.08791509]\n",
      "81 0.0011087825 [0.9622557] [0.08580167]\n",
      "82 0.0010561164 [0.9631631] [0.08373906]\n",
      "83 0.0010059512 [0.96404856] [0.08172601]\n",
      "84 0.0009581642 [0.96491283] [0.07976139]\n",
      "85 0.0009126524 [0.96575636] [0.07784399]\n",
      "86 0.00086930115 [0.9665795] [0.07597266]\n",
      "87 0.0008280067 [0.9673829] [0.07414634]\n",
      "88 0.0007886784 [0.968167] [0.07236391]\n",
      "89 0.00075121573 [0.9689323] [0.07062433]\n",
      "90 0.0007155287 [0.96967906] [0.06892654]\n",
      "91 0.0006815411 [0.970408] [0.06726962]\n",
      "92 0.00064916845 [0.97111934] [0.06565248]\n",
      "93 0.00061833323 [0.9718137] [0.06407426]\n",
      "94 0.000588961 [0.9724912] [0.06253394]\n",
      "95 0.0005609862 [0.9731525] [0.06103067]\n",
      "96 0.00053433835 [0.9737979] [0.05956355]\n",
      "97 0.0005089533 [0.9744277] [0.05813166]\n",
      "98 0.0004847812 [0.9750425] [0.05673424]\n",
      "99 0.00046175314 [0.97564244] [0.05537038]\n",
      "\n",
      "====Test====\n",
      "X:2.5, Y: [2.4944763]\n",
      "X:5, Y: [4.9335823]\n"
     ]
    }
   ],
   "source": [
    "#파이썬의 with 기능으로 세션 생성 & 초기화 \n",
    "with tf.Session() as sess: \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 최적화 수행하는 tain_op 그래프 실행 & 실행시마다 변화하는 손실값 출력 :학습 100번 수행 \n",
    "    for step in range(100):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        \n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "    # 학습에 사용되지 않은 값으로 결과 확인하기 \n",
    "    print(\"\\n====Test====\")\n",
    "    print(\"X:2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))\n",
    "    print(\"X:5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
